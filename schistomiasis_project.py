# -*- coding: utf-8 -*-
"""Schistomiasis Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W9v5Gp_nAyup4NUzxODSGvLJcSnDsc7x

IMPORT LIBRARIES
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, mean_squared_error
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import OneHotEncoder

import json

"""DATA PREPROCESSING"""

# Load data
df = pd.read_excel('/content/Schistomiasis One Health dataset (1).xlsx')

# View the first 5 data
df.head()

# View the last 5 data
df.tail()

# View the information of the dataset
df.info()

# View the numeric description of the dataset
df.describe()

df.drop_duplicates(inplace=True)

# View the shape of the dataset
df.shape

# View the size of the dataset
df.size

# Binary target (if infection data exists)
df['FGS_risk'] = np.where(df['n_ShInfection'] > 0, 1, 0)

# Continuous alternative (if using egg counts)
df['FGS_severity'] = df['mean_ShEgg'] * df['n_female']

# Engineered features
df['female_egg_burden'] = df['mean_ShEgg'] * df['n_female']
df['egg_water_contamination'] = df['mean_ShEgg'] * df['Pop'] / (df['FloatingVeg'] + 1e-6)
df['female_snail_interaction'] = df['n_female'] * df['Bulinus']

# Add child_population_percent and lake_proximity_score columns
df['child_population_percent'] = (df['n_class1'] + df['n_class2'] + df['n_class3'] + df['n_class4']) / df['Pop'] * 100

# Encode microhabitat risk (example mapping)
risk_map = {'shoreline': 3, 'marsh': 2, 'open_water': 1, 'other': 1}
df['microhabitat_risk'] = df['microhabitat'].map(risk_map)

df.info()

# Define columns to drop
drop_columns = [
    'Village', 'Site', 'VillageCode', 'year', # Geospatial/Temporal
    'FGS_severity', # Redundant target/engineered feature
    'microhabitat', # Categorical column causing error
    'n_ShInfection', # Target variable - already in y
    'microhabitat_risk' # Column with all NaNs
]

# Create a processed dataframe by dropping specified columns
df_processed = df.drop(columns=drop_columns)

# Keep only numeric and potentially one-hot encoded columns later
# (Assuming no one-hot encoding is done yet, keeping only numeric for now)
# If one-hot encoding is added, update this selection
numeric_cols_after_drop = df_processed.select_dtypes(include=np.number).columns.tolist()
df_processed = df_processed[numeric_cols_after_drop]

from sklearn.feature_selection import VarianceThreshold

# 1. Drop columns with all nulls (e.g., 'microhabitat_risk')
df = df.dropna(axis=1, how='all')

# 2. Drop low-variance features (optional)
selector = VarianceThreshold(threshold=0.01)
X = df.select_dtypes(include=['int64', 'float64'])
selector.fit(X)
low_var_cols = X.columns[~selector.get_support()].tolist()
df_clean = df.drop(columns=low_var_cols)

print("Remaining columns:", df.columns.tolist())

# Removed redundant scaling step - scaling will be done after train-test split
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
num_cols = ['Depth', 'Pop', 'distance', 'width_shore', 'n_female', 'mean_ShEgg']
df[num_cols] = scaler.fit_transform(df[num_cols])

# Example: Outbreak = 1 if n_ShInfection > threshold
df['outbreak'] = (df['n_ShInfection'] > 50).astype(int)
y = df['outbreak']

df['outbreak'].value_counts()

# Define features (X) and target (y) from the processed dataframe
X = df.drop(columns=['outbreak', 'FGS_risk', 'female_egg_burden', 'Village', 'Site', 'VillageCode']) # Drop target, redundant features, and irrelevant categorical columns
y = df['outbreak'] # Define the target variable

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Identify numeric columns for scaling
numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()

# Create a ColumnTransformer to apply StandardScaler to numeric features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features)
    ],
    remainder='passthrough' # Keep other columns (if any) - should be none after previous steps
)

# Apply preprocessing
X_train_scaled = preprocessor.fit_transform(X_train)
X_test_scaled = preprocessor.transform(X_test)

# Convert back to DataFrame with original column names if needed for interpretability later,
# but for model training, numpy array is fine.
# X_train = pd.DataFrame(X_train_scaled, columns=numeric_features + list(preprocessor.transformers_[0][1].get_feature_names_out())) # Adjust column names if remainder='passthrough' was used
# X_test = pd.DataFrame(X_test_scaled, columns=numeric_features + list(preprocessor.transformers_[0][1].get_feature_names_out())) # Adjust column names if remainder='passthrough' was used

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Identify categorical columns for one-hot encoding (only 'microhabitat' should remain)
categorical_features = X_train.select_dtypes(include='object').columns.tolist()

# Identify numeric columns for scaling
numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()

# Create a ColumnTransformer to apply StandardScaler to numeric features and OneHotEncoder to categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough' # Keep other columns (if any) - should be none after previous steps
)

# Create a pipeline with preprocessing and the model
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                 ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])

# Train the model
model_pipeline.fit(X_train, y_train)

# Evaluate
y_pred = model_pipeline.predict(X_test)
print(classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_pred))

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
# Model Training & Tuning
models = {
    'Logistic Regression': {
        'model': LogisticRegression(max_iter=1000),
        'params': {
            'logisticregression__C': [0.01, 0.1, 1, 10],
            'logisticregression__penalty': ['l2'],
            'logisticregression__class_weight': ['balanced']
        }
    },
    'Decision Tree': {
        'model': DecisionTreeClassifier(),
        'params': {}
    },
    'XGBoost': {
        'model': XGBClassifier(scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1])),
        'params': {
            'xgboost__n_estimators': [50, 100],
            'xgboost__learning_rate': [0.01, 0.1],
            'xgboost__max_depth': [3, 6]
        }
    }
}

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import GridSearchCV

# Identify categorical and numeric features for the ColumnTransformer
categorical_features = X_train.select_dtypes(include='object').columns.tolist()
numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()

# Create a ColumnTransformer to handle preprocessing - drop instead of passthrough
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='drop' # Explicitly drop columns not specified
)

pipelines = {}
for name, cfg in models.items():
    pipelines[name] = GridSearchCV(
        Pipeline([
            ('preprocessor', preprocessor), # Include the preprocessor in the pipeline
            (name.lower().replace(" ", ""), cfg['model'])
        ]),
        param_grid=cfg['params'],
        scoring='f1',
        cv=5,
        n_jobs=-1
    )

import joblib

for name, grid in pipelines.items():
    print(f"Training {name}...")
    grid.fit(X_train, y_train)
    print(f"Best Params for {name}: {grid.best_params_}")
    print(f"Best F1 Score: {grid.best_score_:.4f}\n")
    # Save the best model
    if name == 'XGBoost': # You can change this to save a different model if needed
        joblib.dump(grid.best_estimator_, "best_model.pkl")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

for name, grid in pipelines.items():
    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)

    print(f"=== {name} Performance ===")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    if hasattr(best_model, "predict_proba"):
        y_prob = best_model.predict_proba(X_test)[:, 1]
        print("AUC-ROC:", roc_auc_score(y_test, y_prob))
    print("\n")

import shap

# Example with XGBoost
# Get the best XGBoost model from the pipeline
best_xgb_model = pipelines['XGBoost'].best_estimator_

# Get the preprocessor from the pipeline
preprocessor = best_xgb_model.named_steps['preprocessor']

# Transform X_test using the preprocessor
X_test_transformed = preprocessor.transform(X_test)

# The transformed data is a numpy array, but shap expects a DataFrame with feature names
# We need to get the feature names after preprocessing. This can be a bit tricky with ColumnTransformer.
# A common approach is to manually construct the column names.

# Get feature names from one-hot encoder
onehot_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)

# Get feature names from numeric transformer (these remain the same as original numeric features)
numeric_features_after_transform = numeric_features

# Combine the feature names
all_features_after_transform = list(numeric_features_after_transform) + list(onehot_features)


# Create a DataFrame with transformed data and correct column names
X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=all_features_after_transform)

# Get the trained XGBoost model (the last step in the pipeline)
xgb_model_only = best_xgb_model.named_steps['xgboost']

# Explainer expects the model and the data it was trained on (transformed data)
explainer = shap.Explainer(xgb_model_only)
shap_values = explainer(X_test_transformed_df)
shap.summary_plot(shap_values, X_test_transformed_df)

import gradio as gr
import pandas as pd
import numpy as np
import plotly.express as px
import joblib
from datetime import datetime
import geopandas as gpd
import json

# Load the trained model (replace with your actual model loading code)
try:
    model = joblib.load('best_model.pkl')
except:
    # Create a mock model if file not found
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from xgboost import XGBClassifier
    model = Pipeline([
        ('scaler', StandardScaler()),
        ('xgb', XGBClassifier())
    ])

# Mock data for demonstration
regions = ["North", "South", "East", "West", "Central"]
risk_levels = ["Low", "Medium", "High"]

def generate_mock_data():
    return pd.DataFrame({
        "Region": regions * 3,
        "Risk Level": risk_levels * 5,
        "Cases": np.random.randint(10, 500, 15),
        "Date": [datetime(2023, i%12+1, 15) for i in range(15)],
        "Latitude": np.random.uniform(-1.0, 1.0, 15),
        "Longitude": np.random.uniform(30.0, 35.0, 15),
        "FGS_severity": np.random.uniform(0, 1, 15)
    })

mock_data = generate_mock_data()

# Mock user data
users = pd.DataFrame({
    "Username": ["admin", "health_worker1", "researcher1", "field_agent1", "field_agent2"],
    "Role": ["Administrator", "Health Worker", "Researcher", "Field Agent", "Field Agent"],
    "Last Active": ["Today", "2 days ago", "1 week ago", "Yesterday", "3 days ago"],
    "Status": ["Active", "Active", "Inactive", "Active", "Inactive"]
})

# Mock alert data
alerts = pd.DataFrame({
    "Alert ID": [1, 2, 3, 4],
    "Region": ["North", "South", "East", "West"],
    "Severity": ["High", "Medium", "Low", "High"],
    "Date": ["2023-05-10", "2023-05-12", "2023-05-15", "2023-05-18"],
    "Status": ["New", "In Progress", "Resolved", "New"]
})

# Function to predict risk
def predict_risk(input_data):
    try:
        # Convert input data to DataFrame
        df = pd.DataFrame([input_data])

        # Make prediction (using mock probabilities if model not loaded properly)
        try:
            proba = model.predict_proba(df)[0][1]
        except:
            proba = np.random.uniform(0, 1)

        risk_level = "High" if proba > 0.7 else "Medium" if proba > 0.3 else "Low"
        return {"Risk Probability": float(proba), "Risk Level": risk_level}
    except Exception as e:
        return {"error": str(e)}

# Function to generate report
def generate_report(region, start_date, end_date):
    filtered = mock_data[
        (mock_data["Region"] == region) &
        (mock_data["Date"] >= pd.to_datetime(start_date)) &
        (mock_data["Date"] <= pd.to_datetime(end_date))
    ]

    if len(filtered) == 0:
        return "No data available for the selected criteria"

    report = f"""
    FGS Surveillance Report for {region}
    Period: {start_date} to {end_date}

    Total Cases: {filtered['Cases'].sum()}
    Average FGS Severity: {filtered['FGS_severity'].mean():.2f}
    Highest Risk Area: {filtered.loc[filtered['FGS_severity'].idxmax(), 'Region']}
    """
    return report

# Function to update map
def update_map(selected_region=None):
    gdf = gpd.GeoDataFrame(
        mock_data,
        geometry=gpd.points_from_xy(mock_data.Longitude, mock_data.Latitude)
    )

    if selected_region and selected_region != "All":
        gdf = gdf[gdf["Region"] == selected_region]

    fig = px.scatter_mapbox(
        gdf,
        lat="Latitude",
        lon="Longitude",
        color="Risk Level",
        size="Cases",
        hover_name="Region",
        hover_data=["Cases", "FGS_severity"],
        zoom=5,
        height=500,
        color_discrete_map={"Low": "green", "Medium": "orange", "High": "red"}
    )

    fig.update_layout(
        mapbox_style="open-street-map",
        margin={"r":0,"t":0,"l":0,"b":0}
    )

    return fig

# Function to show trends
def show_trends(selected_region=None, selected_metric="Cases"):
    if selected_region and selected_region != "All":
        df = mock_data[mock_data["Region"] == selected_region]
    else:
        df = mock_data

    df = df.groupby("Date")[selected_metric].sum().reset_index()

    fig = px.line(
        df,
        x="Date",
        y=selected_metric,
        title=f"{selected_metric} Trend Over Time"
    )

    return fig

# Function to add new user
def add_user(username, role, status):
    global users
    new_user = pd.DataFrame({
        "Username": [username],
        "Role": [role],
        "Last Active": ["Today"],
        "Status": [status]
    })
    users = pd.concat([users, new_user], ignore_index=True)
    return "User added successfully", users

# Function to manage alerts
def update_alert_status(alert_id, new_status):
    global alerts
    alerts.loc[alerts["Alert ID"] == alert_id, "Status"] = new_status
    return "Alert status updated", alerts

# Dashboard metrics
def get_dashboard_metrics():
    total_regions = len(mock_data["Region"].unique())
    high_risk = len(mock_data[mock_data["Risk Level"] == "High"])
    coverage_pct = (len(mock_data) / (total_regions * 3)) * 100  # Mock coverage calculation

    total_users = len(users)
    active_users = len(users[users["Status"] == "Active"])
    data_points = len(mock_data)

    return {
        "total_regions": total_regions,
        "high_risk": high_risk,
        "coverage_pct": coverage_pct,
        "total_users": total_users,
        "active_users": active_users,
        "data_points": data_points
    }

# Define the app interface
with gr.Blocks(title="FGS Surveillance Dashboard", theme="soft") as app:
    gr.Markdown("# 🏥 Female Genital Schistosomiasis (FGS) Surveillance Dashboard")

    # Dashboard metrics at the top
    with gr.Row():
        with gr.Column():
            total_regions = gr.Number(label="Total Regions Reached", interactive=False)
        with gr.Column():
            high_risk = gr.Number(label="High Risk Areas", interactive=False)
        with gr.Column():
            coverage_pct = gr.Number(label="Coverage %", interactive=False)
        with gr.Column():
            total_users = gr.Number(label="Total Users", interactive=False)
        with gr.Column():
            active_users = gr.Number(label="Active Users", interactive=False)
        with gr.Column():
            data_points = gr.Number(label="Data Points", interactive=False)

    # Main functionality tabs
    with gr.Tabs():
        # Overview Tab
        with gr.TabItem("Overview"):
            with gr.Tabs():
                with gr.TabItem("Risk Level Assessment"):
                    gr.Markdown("### Risk Level Assessment")
                    with gr.Row():
                        with gr.Column():
                            lake_yn = gr.Dropdown(["Yes", "No"], label="Presence of Lake")
                            fm = gr.Number(label="Female Population")
                            point = gr.Number(label="Water Point Distance (km)")
                            depth = gr.Number(label="Water Depth (m)")
                            bulinus = gr.Number(label="Bulinus Snail Count")
                            biomph = gr.Number(label="Biomph Snail Count")
                        with gr.Column():
                            year = gr.Number(label="Year")
                            pop = gr.Number(label="Total Population")
                            distance = gr.Number(label="Distance to Healthcare (km)")
                            floating_veg = gr.Number(label="Floating Vegetation %")
                            width_shore = gr.Number(label="Shore Width (m)")
                            width_open = gr.Number(label="Open Water Width (m)")
                        with gr.Column():
                            length = gr.Number(label="Water Body Length (km)")
                            circ_score = gr.Number(label="Circulation Score")
                            n_female = gr.Number(label="Number of Females Surveyed")
                            n_class1 = gr.Number(label="Class 1 Infections")
                            n_class2 = gr.Number(label="Class 2 Infections")
                            n_class3 = gr.Number(label="Class 3 Infections")
                    assess_btn = gr.Button("Assess Risk")
                    risk_output = gr.JSON(label="Risk Assessment Results")

                    assess_btn.click(
                        predict_risk,
                        inputs=[
                            lake_yn, fm, point, depth, bulinus, biomph, year, pop,
                            distance, floating_veg, width_shore, width_open, length,
                            circ_score, n_female, n_class1, n_class2, n_class3
                        ],
                        outputs=risk_output
                    )

                with gr.TabItem("Regional Risk Level"):
                    gr.Markdown("### Regional Risk Levels")
                    region_risk_table = gr.Dataframe(
                        value=mock_data[["Region", "Risk Level", "Cases", "FGS_severity"]],
                        interactive=False
                    )

                with gr.TabItem("Generate Report"):
                    gr.Markdown("### Generate Custom Report")
                    report_region = gr.Dropdown(regions + ["All"], label="Select Region")
                    start_date = gr.Textbox(label="Start Date (YYYY-MM-DD)", value="2023-01-01")
                    end_date = gr.Textbox(label="End Date (YYYY-MM-DD)", value="2023-12-31")
                    generate_btn = gr.Button("Generate Report")
                    report_output = gr.Textbox(label="Report", lines=10)

                    generate_btn.click(
                        generate_report,
                        inputs=[report_region, start_date, end_date],
                        outputs=report_output
                    )

        # Mapping Tab
        with gr.TabItem("Mapping"):
            gr.Markdown("### Interactive Risk Mapping")
            map_region = gr.Dropdown(["All"] + regions, label="Filter by Region")
            risk_map = gr.Plot()

            map_region.change(
                update_map,
                inputs=map_region,
                outputs=risk_map
            )

            # Initial load
            app.load(update_map, inputs=None, outputs=risk_map)

        # Trends Tab
        with gr.TabItem("Trends"):
            gr.Markdown("### Disease Trends Over Time")
            trend_region = gr.Dropdown(["All"] + regions, label="Filter by Region")
            trend_metric = gr.Dropdown(["Cases", "FGS_severity"], label="Select Metric")
            trend_plot = gr.Plot()

            trend_region.change(
                show_trends,
                inputs=[trend_region, trend_metric],
                outputs=trend_plot
            )

            trend_metric.change(
                show_trends,
                inputs=[trend_region, trend_metric],
                outputs=trend_plot
            )

            # Initial load
            app.load(
                show_trends,
                inputs=[gr.State("All"), gr.State("Cases")],
                outputs=trend_plot
            )

        # Survey Tab
        with gr.TabItem("Survey"):
            gr.Markdown("### Data Collection Survey")
            with gr.Row():
                with gr.Column():
                    survey_region = gr.Dropdown(regions, label="Region")
                    survey_date = gr.Textbox(label="Date (YYYY-MM-DD)", value=datetime.today().strftime('%Y-%m-%d'))
                    cases = gr.Number(label="Number of Cases")
                    severity = gr.Slider(0, 1, label="Average Severity Score")
                with gr.Column():
                    water_source = gr.Dropdown(["Lake", "River", "Pond", "Other"], label="Main Water Source")
                    sanitation = gr.Slider(0, 100, label="Sanitation Coverage (%)")
                    health_access = gr.Slider(0, 100, label="Healthcare Access (%)")
                    awareness = gr.Slider(0, 100, label="Awareness Level (%)")
            submit_survey = gr.Button("Submit Survey Data")
            survey_output = gr.Textbox(label="Submission Status")

            def submit_survey_data(region, date, cases, severity, water_source, sanitation, health_access, awareness):
                global mock_data
                new_entry = {
                    "Region": region,
                    "Date": pd.to_datetime(date),
                    "Cases": cases,
                    "FGS_severity": severity,
                    "Latitude": np.random.uniform(-1.0, 1.0),
                    "Longitude": np.random.uniform(30.0, 35.0),
                    "Risk Level": "High" if severity > 0.7 else "Medium" if severity > 0.3 else "Low"
                }
                mock_data = mock_data.append(new_entry, ignore_index=True)
                return "Survey data submitted successfully", mock_data[["Region", "Date", "Cases", "Risk Level"]]

            submit_survey.click(
                submit_survey_data,
                inputs=[survey_region, survey_date, cases, severity, water_source, sanitation, health_access, awareness],
                outputs=[survey_output, gr.State(mock_data)]
            )

        # Admin Tab
        with gr.TabItem("Admin"):
            gr.Markdown("### Administration Panel")
            with gr.Tabs():
                with gr.TabItem("User Management"):
                    gr.Markdown("#### Current Users")
                    user_table = gr.Dataframe(value=users, interactive=False)

                    gr.Markdown("#### Add New User")
                    with gr.Row():
                        new_username = gr.Textbox(label="Username")
                        new_role = gr.Dropdown(["Administrator", "Health Worker", "Researcher", "Field Agent"], label="Role")
                        new_status = gr.Dropdown(["Active", "Inactive"], label="Status")
                    add_user_btn = gr.Button("Add User")
                    user_status = gr.Textbox(label="Status")

                    add_user_btn.click(
                        add_user,
                        inputs=[new_username, new_role, new_status],
                        outputs=[user_status, user_table]
                    )

                with gr.TabItem("Alert Management"):
                    gr.Markdown("#### Current Alerts")
                    alert_table = gr.Dataframe(value=alerts, interactive=False)

                    gr.Markdown("#### Update Alert Status")
                    alert_id = gr.Number(label="Alert ID")
                    alert_status = gr.Dropdown(["New", "In Progress", "Resolved"], label="New Status")
                    update_alert_btn = gr.Button("Update Status")
                    alert_status_output = gr.Textbox(label="Status")

                    update_alert_btn.click(
                        update_alert_status,
                        inputs=[alert_id, alert_status],
                        outputs=[alert_status_output, alert_table]
                    )

                with gr.TabItem("Data Management"):
                    gr.Markdown("#### Current Data")
                    data_table = gr.Dataframe(value=mock_data[["Region", "Date", "Cases", "Risk Level"]], interactive=False)

                    gr.Markdown("#### Upload New Data")
                    data_upload = gr.File(label="Upload CSV File")
                    upload_btn = gr.Button("Process Upload")
                    upload_status = gr.Textbox(label="Status")

                    def process_upload(file):
                        try:
                            # In a real app, you would process the uploaded file
                            return "Data uploaded successfully. Refresh to see updates."
                        except Exception as e:
                            return f"Error processing file: {str(e)}"

                    upload_btn.click(
                        process_upload,
                        inputs=data_upload,
                        outputs=upload_status
                    )

    # Load dashboard metrics on startup
    def load_metrics():
        metrics = get_dashboard_metrics()
        return [
            metrics["total_regions"],
            metrics["high_risk"],
            metrics["coverage_pct"],
            metrics["total_users"],
            metrics["active_users"],
            metrics["data_points"]
        ]

    app.load(
        load_metrics,
        outputs=[total_regions, high_risk, coverage_pct, total_users, active_users, data_points]
    )

# Launch the app
if __name__ == "__main__":
    app.launch(server_name="0.0.0.0", server_port=7870)