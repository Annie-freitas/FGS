# -- coding: utf-8 --
"""OH-FGS  Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c7vmzYncGBP226UiQpgEo9lBgZeJD1wQ

IMPORT LIBRARIES
"""

# 2. Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# 3. Load dataset (upload your Excel file when prompted)
import streamlit as st
import pandas as pd

uploaded_file = st.file_uploader("Upload OH-FGS Excel file", type=["xlsx"])
if uploaded_file:
    df = pd.read_excel(uploaded_file)
    st.write(df.head())
else:
    st.warning("Please upload an Excel file")

# 4. Feature Selection (One Health Approach)
# Key variables from your dataset + engineered features
one_health_features = [
    # Human Health
    'n_ShInfection',       # Primary target (binary: outbreak if >50)
    'mean_ShEgg',          # Egg burden (continuous)
    'n_female',            # FGS risk group
    'Pop',                 # Population density

    # Environmental
    'LakeYN',              # Lake presence (binary)
    'distance',            # Proximity to water (meters)
    'FloatingVeg',         # Snail habitat indicator
    'Depth',               # Water depth (snail survival)
    'width_shore',         # Shoreline width (transmission zone)

    # Animal/Vector
    'Bulinus',             # Snail vector count
    'Biomph',              # Alternative snail vector
    'circ_score'           # Water body circularity (snail habitat)
]

# Create new features
df['water_contact_risk'] = np.where(df['distance'] < 1000, 1, 0)  # High risk if <1km to water
df['snail_density'] = df['Bulinus'] + df['Biomph']               # Total snail load

# 5. Prepare data
X = df[one_health_features]
y = np.where(df['n_ShInfection'] > 50, 1, 0)  # Binary outbreak target (1 if >50 cases)

# 6. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# 7. Train model (Random Forest)
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_model.fit(X_train, y_train)

dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

xg_model = XGBClassifier(random_state=42)
xg_model.fit(X_train, y_train)

lg_model = LogisticRegression(random_state=42)
lg_model.fit(X_train, y_train)

# 8. Evaluate

#Evaluate Random Forest model
y_pred_rf = rf_model.predict(X_test)
print("Random Forest Model Evaluation:")
print(classification_report(y_test, y_pred_rf))
print(f"AUC-ROC: {roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1]):.2f}")

# Evaluate Decision Tree model
y_pred_dt = dt_model.predict(X_test)
print("Decision Tree Model Evaluation:")
print(classification_report(y_test, y_pred_dt))
print(f"AUC-ROC: {roc_auc_score(y_test, dt_model.predict_proba(X_test)[:, 1]):.2f}")

# Evaluate XGBoost model
y_pred_xg = xg_model.predict(X_test)
print("\nXGBoost Model Evaluation:")
print(classification_report(y_test, y_pred_xg))
print(f"AUC-ROC: {roc_auc_score(y_test, xg_model.predict_proba(X_test)[:, 1]):.2f}")

# Evaluate Logistic Regression model
y_pred_lg = lg_model.predict(X_test)
print("\nLogistic Regression Model Evaluation:")
print(classification_report(y_test, y_pred_lg))
print(f"AUC-ROC: {roc_auc_score(y_test, lg_model.predict_proba(X_test)[:, 1]):.2f}")

# 9. Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=rf_model.feature_importances_, y=one_health_features)
plt.title("One Health Feature Importance")
plt.show()

import seaborn as sns
df['risk_score'] = lg_model.predict_proba(X)[:, 1]

# Now, the heatmap code should work
# Pivot table: Village x Site risk scores
heatmap_data = df.pivot_table(index='Village', columns='Site',
                             values='risk_score', aggfunc='mean')

# Plot
plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data, annot=True, cmap="YlOrRd",
            linewidths=0.5, cbar_kws={'label': 'Risk Score'})
plt.title("Schisto Risk by Village and Site")
plt.show()

import pickle

# Save the model
chosen_model = lg_model
with open('schisto_risk_model.pkl', 'wb') as f:
    pickle.dump(chosen_model, f)

print("LGR model saved as schisto_risk_model.pkl")

def send_stakeholder_alerts(village_data, risk_level):
    # Single recipient (modify this to your target email)
    recipient_email = "freitasannie7@gmail.com"

    # Compose the alert message
    message = f"""
    üö® SCHISTOSOMIASIS ALERT - {village_data['Village']} üö®
    Risk Level: {risk_level}
    Risk Score: {village_data['risk_score']:.2f}

    üìç Location: Site {village_data['Site']}
    üìä Metrics:
    - Infected Individuals: {village_data['n_ShInfection']}
    - Bulinus Snail Count: {village_data['Bulinus']}
    - Water Contact Risk: {village_data['distance']}m from village

    ‚ö† Recommended Actions:
    {generate_intervention_plan(village_data)}

    üìÖ Next Assessment: {pd.Timestamp.now() + pd.Timedelta(days=7)}
    """

    # Send only one email
    send_email(recipient_email, f"SCHISTO ALERT: {risk_level} Risk Detected", message)

    # Optional: Log the alert (no SMS or emergency protocols)
    print(f"üìß Alert sent to {recipient_email} for {village_data['Village']}")

import pandas as pd
import numpy as np

# Track Intervention Effectiveness
def evaluate_impact(baseline, follow_up):
    improvement = {
        'Risk Reduction': baseline['risk_score'] - follow_up['risk_score'],
        'Case Reduction': (baseline['n_ShInfection'] - follow_up['n_ShInfection'])/baseline['n_ShInfection']*100,
        'Snail Reduction': (baseline['Bulinus'] - follow_up['Bulinus'])/baseline['Bulinus']*100
    }
    return improvement

# Example Report
# Replace 'Diokhor' with an actual Village name from your dataset (e.g., 'Diokhor ')
baseline = df[df['Village']=='Diokhor '].iloc[0]
follow_up = baseline.copy()  # Replace with actual follow-up data
follow_up.update({'risk_score': baseline['risk_score']*0.6,
                 'n_ShInfection': baseline['n_ShInfection']*0.7,
                 'Bulinus': baseline['Bulinus']*0.5})

print("üìà Intervention Effectiveness:")
st.dataframe(pd.DataFrame([evaluate_impact(baseline, follow_up)]))

# -- coding: utf-8 --
import streamlit as st
import pandas as pd
import numpy as np
import pickle
from datetime import datetime
import tempfile
import os

# Set page config
st.set_page_config(
    page_title="OH-FGS Risk Prediction",
    page_icon="ü¶†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main {background-color: #f5f5f5;}
    .stAlert {padding: 20px; border-radius: 10px;}
    .high-risk {background-color: #ffcccc; padding: 10px; border-radius: 5px;}
    .medium-risk {background-color: #fff3cd; padding: 10px; border-radius: 5px;}
    .low-risk {background-color: #d4edda; padding: 10px; border-radius: 5px;}
    .header {color: #2c3e50;}
    .stTextInput>div>div>input {background-color: #f0f2f6;}
</style>
""", unsafe_allow_html=True)

# Load model
@st.cache_resource
def load_model():
    try:
        with open('schisto_risk_model.pkl', 'rb') as f:
            model = pickle.load(f)
        return model
    except Exception as e:
        st.error(f"Failed to load model: {str(e)}")
        return None

model = load_model()

# Expected features
FEATURES = [
    'n_ShInfection', 'mean_ShEgg', 'n_female', 'Pop', 'LakeYN',
    'distance', 'FloatingVeg', 'Depth', 'width_shore', 'Bulinus',
    'Biomph', 'circ_score', 'Village', 'Site', 'Date'
]

# Initialize session state for data storage
if 'saved_data' not in st.session_state:
    st.session_state.saved_data = pd.DataFrame(columns=FEATURES + ['Risk_Level', 'Risk_Probability'])

# Data entry form
def individual_entry_form():
    with st.form("individual_entry"):
        st.subheader("Individual Data Entry")
        
        cols = st.columns(3)
        with cols[0]:
            village = st.text_input("Village Name", "Diokhor")
            site = st.text_input("Site Number", "1")
            date = st.date_input("Date", datetime.today())
            n_ShInfection = st.number_input("Number of Infections", min_value=0, value=10)
            mean_ShEgg = st.number_input("Mean Egg Count", min_value=0.0, value=5.2)
        
        with cols[1]:
            n_female = st.number_input("Number of Females", min_value=0, value=15)
            pop = st.number_input("Population", min_value=0, value=200)
            lake_yn = st.selectbox("Lake Present", ["Yes", "No"], index=0)
            distance = st.number_input("Distance to Water (m)", min_value=0, value=500)
            floating_veg = st.selectbox("Floating Vegetation", ["Low", "Medium", "High"], index=1)
        
        with cols[2]:
            depth = st.number_input("Water Depth (m)", min_value=0.0, value=1.5)
            width_shore = st.number_input("Shore Width (m)", min_value=0.0, value=10.0)
            bulinus = st.number_input("Bulinus Count", min_value=0, value=20)
            biomph = st.number_input("Biomph Count", min_value=0, value=5)
            circ_score = st.number_input("Circularity Score", min_value=0.0, max_value=1.0, value=0.7)
        
        submitted = st.form_submit_button("Submit & Predict")
        
        if submitted:
            # Prepare data
            data = {
                'Village': village,
                'Site': site,
                'Date': date.strftime('%Y-%m-%d'),
                'n_ShInfection': n_ShInfection,
                'mean_ShEgg': mean_ShEgg,
                'n_female': n_female,
                'Pop': pop,
                'LakeYN': 1 if lake_yn == "Yes" else 0,
                'distance': distance,
                'FloatingVeg': ["Low", "Medium", "High"].index(floating_veg) + 1,
                'Depth': depth,
                'width_shore': width_shore,
                'Bulinus': bulinus,
                'Biomph': biomph,
                'circ_score': circ_score
            }
            
            # Make prediction
            features = pd.DataFrame([data])[FEATURES[:-3]]  # Exclude metadata columns
            proba = model.predict_proba(features)[0][1]
            risk_level = "High" if proba > 0.7 else "Medium" if proba > 0.3 else "Low"
            
            # Add to session data
            data.update({
                'Risk_Level': risk_level,
                'Risk_Probability': proba
            })
            
            new_row = pd.DataFrame([data])
            st.session_state.saved_data = pd.concat([st.session_state.saved_data, new_row], ignore_index=True)
            
            # Show result
            st.success("Data submitted successfully!")
            st.write(f"Predicted Risk: **{risk_level}** (Probability: {proba:.2%})")

# File upload function
def bulk_upload():
    st.subheader("Bulk Data Upload")
    uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])
    
    if uploaded_file is not None:
        try:
            new_data = pd.read_csv(uploaded_file)
            
            # Validate columns
            missing_cols = [col for col in FEATURES[:-3] if col not in new_data.columns]
            if missing_cols:
                st.error(f"Missing required columns: {', '.join(missing_cols)}")
                return
            
            # Make predictions
            features = new_data[FEATURES[:-3]]
            probas = model.predict_proba(features)[:, 1]
            risk_levels = ["High" if p > 0.7 else "Medium" if p > 0.3 else "Low" for p in probas]
            
            # Add to session data
            new_data['Risk_Level'] = risk_levels
            new_data['Risk_Probability'] = probas
            
            # Add missing metadata columns if needed
            for col in ['Village', 'Site', 'Date']:
                if col not in new_data.columns:
                    new_data[col] = "Unknown"
            
            st.session_state.saved_data = pd.concat([st.session_state.saved_data, new_data], ignore_index=True)
            st.success(f"Successfully added {len(new_data)} records!")
            
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")

# DHIS2 export format
def format_for_dhis2(df):
    dhis2_format = df.copy()
    
    # Add required DHIS2 columns
    dhis2_format['orgUnit'] = dhis2_format['Village']
    dhis2_format['period'] = pd.to_datetime(dhis2_format['Date']).dt.strftime('%Y%m')
    dhis2_format['dataSet'] = 'SCHISTO_RISK'
    
    # Map to DHIS2 data elements
    indicator_mapping = {
        'n_ShInfection': 'SCHISTO_CASES',
        'Bulinus': 'SNAIL_COUNT_BULINUS',
        'Risk_Level': 'RISK_LEVEL',
        'Risk_Probability': 'RISK_SCORE'
    }
    
    # Create DHIS2 compatible format
    melted = pd.melt(dhis2_format, 
                    id_vars=['orgUnit', 'period', 'dataSet', 'Site'],
                    value_vars=list(indicator_mapping.keys()),
                    var_name='dataElement',
                    value_name='value')
    
    # Map to DHIS2 element codes
    melted['dataElement'] = melted['dataElement'].map(indicator_mapping)
    
    return melted[['dataElement', 'period', 'orgUnit', 'dataSet', 'Site', 'value']]

# Main app
def main():
    st.title("üåç OH-FGS Schistosomiasis Risk Prediction")
    
    tab1, tab2, tab3 = st.tabs(["Individual Entry", "Bulk Upload", "Saved Data"])
    
    with tab1:
        individual_entry_form()
    
    with tab2:
        bulk_upload()
    
    with tab3:
        st.subheader("Collected Data")
        
        if not st.session_state.saved_data.empty:
            st.dataframe(st.session_state.saved_data)
            
            # Export options
            st.subheader("Export Data")
            
            col1, col2 = st.columns(2)
            with col1:
                # CSV export
                csv = st.session_state.saved_data.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="Download Full Data (CSV)",
                    data=csv,
                    file_name=f"schisto_data_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )
            
            with col2:
                # DHIS2 export
                if st.button("Prepare DHIS2 Export"):
                    dhis2_data = format_for_dhis2(st.session_state.saved_data)
                    csv_dhis2 = dhis2_data.to_csv(index=False).encode('utf-8')
                    
                    st.download_button(
                        label="Download DHIS2 Format",
                        data=csv_dhis2,
                        file_name=f"schisto_dhis2_export_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )
                    st.write("DHIS2 compatible format ready for import")
        else:
            st.info("No data collected yet. Use the Individual Entry or Bulk Upload tabs to add data.")

if __name__ == "__main__":
    main()
